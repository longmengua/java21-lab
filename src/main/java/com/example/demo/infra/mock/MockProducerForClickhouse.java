package com.example.demo.infra.mock;

import com.example.demo.infra.props.KafkaProps;
import com.example.demo.infra.props.TopicsProps;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.stereotype.Component;

import java.time.Instant;
import java.util.Properties;
import java.util.UUID;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicReference;
import java.util.concurrent.ThreadLocalRandom;

/**
 * <p>Kafka æ¨¡æ“¬è³‡æ–™ç”Ÿç”¢å™¨</p>
 *
 * å•Ÿå‹•æ¢ä»¶ï¼š
 *   1) application.yml: mock.kafka.clickhouse=true
 *   2) application.yml: kafka.enabled=true
 *   3) å·²è¨­å®š kafka.bootstrap-servers
 *   4) å·²è¨­å®š topics.events
 * è¡Œç‚ºï¼š
 *   - å•Ÿå‹•å¾ŒèƒŒæ™¯å›ºå®šæ¯ç§’é€å‡º rps ç­†è³‡æ–™åˆ° events topicï¼Œç¸½è¨ˆ seconds ç§’
 *   - çµæŸå¾Œ flush ä¸¦é—œé–‰ Producer
 */
@Component
@ConditionalOnProperty(name = "mock.kafka.clickhouse", havingValue = "true")
@Slf4j
public class MockProducerForClickhouse {

    /** Kafka é€£ç·šèˆ‡ Producer æ¸¬è©¦åƒæ•¸ï¼ˆç”± application.yml ç¶å®šï¼‰ */
    private final KafkaProps kafkaProps;

    /** Topics è¨­å®šï¼ˆç”± application.yml ç¶å®šï¼‰ */
    private final TopicsProps topics;

    /** JSON åºåˆ—åŒ–å·¥å…·ï¼ˆç”¢æ¸¬è©¦ JSON å­—ä¸²ï¼‰ */
    private static final ObjectMapper mapper = new ObjectMapper();

    /** Kafka Producer å¯¦ä¾‹ï¼ˆåœ¨ PostConstruct å»ºç«‹ï¼ŒPreDestroy é—œé–‰ï¼‰ */
    private Producer<String, String> producer;

    /** æ¯ç§’æ’ç¨‹å™¨ï¼ˆé•·ç”Ÿå‘½é€±æœŸï¼Œèˆ‡ Bean åŒç”ŸåŒæ»…ï¼‰ */
    private ScheduledExecutorService scheduler;

    /** ä¿å­˜ scheduleAtFixedRate å›å‚³çš„ futureï¼Œå¥½åœ¨åˆ°æ™‚å€™å–æ¶ˆè‡ªå·± */
    private final AtomicReference<ScheduledFuture<?>> scheduledRef = new AtomicReference<>();

    /** ç”¨æ–¼è¨˜éŒ„å•Ÿå‹•çš„èƒŒæ™¯ã€Œå•Ÿå‹•å™¨ã€åŸ·è¡Œç·’ï¼ˆå¯é¸ï¼Œä¸»è¦ç‚ºäº†èˆ‡åŸé‚è¼¯å°é½Šï¼‰ */
    private Thread launcher;

    public MockProducerForClickhouse(KafkaProps kafkaProps, TopicsProps topics) {
        this.kafkaProps = kafkaProps;
        this.topics = topics;
    }

    /**
     * Bean åˆå§‹åŒ–ï¼šæª¢æŸ¥è¨­å®š â†’ å»º Producer â†’ å•Ÿå‹•å›ºå®šé »ç‡é€è³‡æ–™
     */
    @PostConstruct
    public void startProducing() {
        // ====== 0) å•Ÿå‹•å‰æª¢æŸ¥æ¢ä»¶ ======
        if (!kafkaProps.isEnabled()) {
            log.warn("Kafka æœªå•Ÿç”¨ï¼ˆkafka.enabled=falseï¼‰ï¼Œè·³é Mock Producer");
            return;
        }

        String bootstrap = kafkaProps.getBootstrapServers();
        if (bootstrap == null || bootstrap.isBlank()) {
            log.warn("ç¼ºå°‘ kafka.bootstrap-serversï¼Œè·³é Mock Producer");
            return;
        }

        String topicName = topics.getEvents();
        if (topicName == null || topicName.isBlank()) {
            throw new IllegalStateException("ç¼ºå°‘ topics.events é…ç½®ï¼Œè«‹åœ¨ application.yml è¨­å®š topics.events");
        }

        int rps = (kafkaProps.getProducer() != null) ? kafkaProps.getProducer().getRecordsPerSecond() : 0;
        int seconds = (kafkaProps.getProducer() != null) ? kafkaProps.getProducer().getDurationSeconds() : 0;
        if (rps <= 0 || seconds <= 0) {
            log.info("æ¸¬è©¦åƒæ•¸ä¸ç”Ÿæ•ˆï¼ˆrecordsPerSecond={}, durationSeconds={}ï¼‰ï¼Œè·³é Mock Producer", rps, seconds);
            return;
        }

        // ====== 1) å»ºç«‹ Kafka Producer ======
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrap);
        props.put(ProducerConfig.ACKS_CONFIG, "1"); // åƒ… leader ackï¼Œé™ä½å»¶é²ï¼ˆæ¨¡æ“¬è³‡æ–™ä¸éœ€é«˜å¯é ï¼‰
        props.put(ProducerConfig.LINGER_MS_CONFIG, "5"); // æœ€å¤šå»¶é² 5ms æ‰ç™¼é€ï¼ˆä¿ƒé€² batchï¼‰
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, "32768"); // 32KB æ‰¹æ¬¡
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4"); // å£“ç¸®æåå
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "false"); // æ¨¡æ“¬è³‡æ–™ï¼Œä¸æ±‚ EO
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        this.producer = new KafkaProducer<>(props);

        // ====== 2) å•Ÿå‹•å›ºå®šé »ç‡é€è³‡æ–™ï¼ˆæ¯ç§’ï¼‰ ======
        // ç”¨å–®åŸ·è¡Œç·’æ’ç¨‹å™¨ï¼›è¨­ç‚º daemonï¼Œéš¨ JVM çµæŸ
        this.scheduler = Executors.newSingleThreadScheduledExecutor(r -> {
            Thread t = new Thread(r, "mock-kafka-producer");
            t.setDaemon(true);
            return t;
        });

        final long endAt = System.currentTimeMillis() + seconds * 1000L;
        log.info("ğŸš€ Mock Kafka Producer ON. Sending {} records/sec to topic '{}' for {} sec...", rps, topicName, seconds);

        // å•Ÿå‹•å™¨ï¼šä¸»è¦æ˜¯ç‚ºäº†èˆ‡åŸæœ¬ã€Œå•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’ã€çš„æ…£ä¾‹ä¸€è‡´ï¼Œå¯çœç•¥
        this.launcher = new Thread(() -> {
            // æ ¸å¿ƒå·¥ä½œï¼šæ¯ç§’åŸ·è¡Œä¸€æ¬¡
            ScheduledFuture<?> future = scheduler.scheduleAtFixedRate(() -> {
                long now = System.currentTimeMillis();

                // åˆ°æ™‚é–“äº†ï¼šå–æ¶ˆè‡ªå·± â†’ é—œé–‰æ’ç¨‹å™¨ â†’ flush â†’ å®Œæˆ
                if (now >= endAt) {
                    try {
                        ScheduledFuture<?> self = scheduledRef.get();
                        if (self != null && !self.isCancelled()) {
                            self.cancel(false); // å–æ¶ˆå¾ŒçºŒè§¸ç™¼ï¼ˆä¸æ‰“æ–·æ­£åœ¨è·‘çš„é€™æ¬¡ä»»å‹™ï¼‰
                        }
                        scheduler.shutdown(); // åœæ­¢æ¥å—æ–°ä»»å‹™

                        // ç­‰å¾…æ’ç¨‹å™¨å„ªé›…é—œé–‰
                        boolean terminated = scheduler.awaitTermination(2, TimeUnit.SECONDS);
                        if (!terminated) {
                            log.warn("Mock Kafka Producer scheduler did not terminate within timeout");
                        }
                    } catch (InterruptedException ie) {
                        // æ¢å¾©ä¸­æ–·ç‹€æ…‹ï¼Œé¿å…åæ‰ä¸­æ–·
                        Thread.currentThread().interrupt();
                        log.warn("Scheduler termination interrupted", ie);
                    } catch (Exception e) {
                        log.warn("Scheduler shutdown error: {}", e.getMessage(), e);
                    }

                    // é€å®Œå°¾ç«¯è³‡æ–™
                    try {
                        producer.flush();
                    } catch (Exception e) {
                        log.warn("Producer flush error on finish: {}", e.getMessage(), e);
                    }

                    log.info("âœ… Mock Kafka Producer finished.");
                    return; // çµæŸé€™æ¬¡åŸ·è¡Œ
                }

                // åœ¨æœ¬ç§’å…§é€å‡º rps ç­†
                for (int i = 0; i < rps; i++) {
                    try {
                        String key = String.valueOf(randomUserId()); // åŒ userId â†’ åŒ partitionï¼ˆåˆ©æ–¼æ¸¬è©¦å¯é æœŸæ€§ï¼‰
                        String json = generateJson(key);
                        producer.send(new ProducerRecord<>(topicName, key, json), (md, ex) -> {
                            if (ex != null) {
                                log.error("Produce failed: {}", ex.getMessage(), ex);
                            }
                        });
                    } catch (Exception e) {
                        log.error("Build record failed: {}", e.getMessage(), e);
                    }
                }
            }, 0, 1, TimeUnit.SECONDS);

            // å­˜èµ·ä¾†ï¼Œæ–¹ä¾¿åœ¨ã€Œåˆ°æ™‚é–“ã€æ™‚è‡ªæˆ‘å–æ¶ˆ
            scheduledRef.set(future);
        }, "mock-kafka-producer-launcher");

        this.launcher.setDaemon(true);
        this.launcher.start();
    }

    /**
     * Bean éŠ·æ¯€ï¼šç¢ºä¿ scheduler èˆ‡ producer éƒ½è¢«å„ªé›…é—œé–‰
     */
    @PreDestroy
    public void shutdown() {
        // 1) åœæ’ç¨‹å™¨
        if (scheduler != null && !scheduler.isShutdown()) {
            try {
                ScheduledFuture<?> f = scheduledRef.get();
                if (f != null && !f.isCancelled()) {
                    f.cancel(false);
                }
                scheduler.shutdown(); // åœæ­¢æ¥æ–°ä»»å‹™
                boolean terminated = scheduler.awaitTermination(2, TimeUnit.SECONDS);
                if (!terminated) {
                    log.warn("Scheduler did not terminate in time, forcing shutdownNow()");
                    scheduler.shutdownNow();
                }
            } catch (InterruptedException ie) {
                Thread.currentThread().interrupt();
                log.warn("Interrupted while awaiting scheduler termination", ie);
                scheduler.shutdownNow();
            } catch (Exception e) {
                log.warn("Error during scheduler shutdown: {}", e.getMessage(), e);
            }
        }

        // 2) ç­‰å¾…å•Ÿå‹•å™¨åŸ·è¡Œç·’ï¼ˆè‹¥ä»å­˜æ´»ï¼‰
        try {
            if (launcher != null && launcher.isAlive()) {
                launcher.join(2000);
            }
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            log.warn("Interrupted while joining launcher thread", ie);
        }

        // 3) é—œ Producer
        if (producer != null) {
            try {
                producer.flush();
            } catch (Exception e) {
                log.warn("Producer flush error on shutdown: {}", e.getMessage(), e);
            }
            try {
                producer.close();
            } catch (Exception e) {
                log.warn("Producer close error: {}", e.getMessage(), e);
            }
        }
    }

    /** ç”¢ç”Ÿéš¨æ©Ÿ userIdï¼ˆ0 ~ 9999ï¼‰ */
    private static long randomUserId() {
        return ThreadLocalRandom.current().nextLong(10_000L);
    }

    /** ç”¢ç”Ÿæ¸¬è©¦ JSON è³‡æ–™ */
    private static String generateJson(String userId) throws Exception {
        var node = mapper.createObjectNode()
                .put("user_id", userId)
                .put("action", UUID.randomUUID().toString().substring(0, 8))
                .put("event_time", Instant.now().toString());
        return mapper.writeValueAsString(node);
    }
}
