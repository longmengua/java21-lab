package com.example.demo.application.scheduler;

import com.example.demo.application.event.KafkaToClickhouseEvent;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.annotation.PostConstruct;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.RichMapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import org.springframework.stereotype.Component;

import java.sql.*;
import java.time.Instant;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

/***
 * å•Ÿå‹• Spring Boot æ‡‰ç”¨
 *     â”‚
 *     â”œâ”€ æƒæåˆ° KafkaToClickHouseJob è¢«è¨»è§£ç‚º @Component
 *     â”‚
 *     â”œâ”€ å¯¦ä¾‹åŒ– KafkaToClickHouseJob Bean
 *     â”‚
 *     â””â”€ èª¿ç”¨ @PostConstruct æ¨™è¨»çš„æ–¹æ³• â†’ startFlinkJob()
 *               â”‚
 *               â””â”€â”€> Flink job å°±é–‹å§‹åŸ·è¡Œï¼ˆéåŒæ­¥ï¼‰
 * */
@Component
public class KafkaToClickHouseJob {

    @PostConstruct
    public void startFlinkJob() throws Exception {
        System.out.println("âœ… Starting Flink job...");

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // âœ… è¨­å®š Flink å…¨åŸŸä½µè¡Œåº¦ï¼ˆparallelismï¼‰
        env.setParallelism(2); // å¯ä¾ CPU èª¿æ•´ï¼Œä¾‹å¦‚ 2 æ ¸å¿ƒå°±è¨­ 2

        // âœ… Kafka Source (new API)
        KafkaSource<String> source = KafkaSource.<String>builder()
                .setBootstrapServers("single-kafka.kafka.orb.local:9092")
                .setTopics("events")
                .setGroupId("flink-consumer")
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .setStartingOffsets(OffsetsInitializer.earliest())
                .build();

        // âœ… å»ºç«‹è³‡æ–™æµç¨‹
        DataStream<String> raw = env.fromSource(source, WatermarkStrategy.noWatermarks(), "KafkaSource");

        // âœ… JSON è½‰äº‹ä»¶ç‰©ä»¶
        DataStream<KafkaToClickhouseEvent> parsed = raw.map(new JsonToEvent());

        // âœ… å¯«å…¥ ClickHouse
        parsed.addSink(new ClickHouseSink());

        env.executeAsync("Spring Boot Flink KafkaToClickHouse (KafkaSource)");
    }

    // âœ… JSON å­—ä¸²è½‰äº‹ä»¶
    static class JsonToEvent extends RichMapFunction<String, KafkaToClickhouseEvent> {
        private final ObjectMapper mapper = new ObjectMapper();

        @Override
        public KafkaToClickhouseEvent map(String json) throws Exception {
            JsonNode node = mapper.readTree(json);
            return new KafkaToClickhouseEvent(
                    node.get("user_id").asLong(),
                    node.get("action").asText(),
                    Instant.parse(node.get("event_time").asText())
            );
        }
    }

    // âœ… ClickHouse Sink with batch
    static class ClickHouseSink extends RichSinkFunction<KafkaToClickhouseEvent> {
        private transient Connection conn;
        private transient PreparedStatement stmt;
        private final List<KafkaToClickhouseEvent> buffer = new ArrayList<>();
        private static final int BATCH_SIZE = 100;

        private transient ScheduledExecutorService monitorExecutor;

        @Override
        public void open(Configuration parameters) throws Exception {
            conn = DriverManager.getConnection("jdbc:clickhouse://localhost:8123/default");
            stmt = conn.prepareStatement("INSERT INTO flink_sink (user_id, action, event_time) VALUES (?, ?, ?)");

            // âœ… å•Ÿå‹•èƒŒæ™¯ç›£æ§ä»»å‹™ï¼Œæ¯ 5 ç§’æª¢æŸ¥ buffer æ˜¯å¦ç‚ºç©º
            monitorExecutor = Executors.newSingleThreadScheduledExecutor();
            monitorExecutor.scheduleAtFixedRate(() -> {
                if (buffer.isEmpty()) {
                    System.out.println("ğŸŸ¡ Still waiting for data from Kafka...");
                } else {
                    System.out.println("ğŸ“¦ Buffer size: " + buffer.size());
                }
            }, 0, 5, TimeUnit.SECONDS);
        }

        @Override
        public void invoke(KafkaToClickhouseEvent event, Context context) throws Exception {
            buffer.add(event);
            if (buffer.size() >= BATCH_SIZE) {
                flush();
            }
        }

        private void flush() throws SQLException {
            try {
                for (KafkaToClickhouseEvent e : buffer) {
                    stmt.setLong(1, e.getUserId());
                    stmt.setString(2, e.getAction());
                    stmt.setTimestamp(3, Timestamp.from(e.getEventTime()));
                    stmt.addBatch();
                }
                int[] result = stmt.executeBatch();
                System.out.printf("âœ… ClickHouse batch insert success: %d records at %s%n",
                        result.length, Instant.now()); // æˆåŠŸå¯«å…¥çš„ç­†æ•¸èˆ‡æ™‚é–“
            } catch (SQLException ex) {
                ex.printStackTrace(); // å¯ä»¥æ›æˆ log
            } finally {
                buffer.clear();
            }
        }

        @Override
        public void close() throws Exception {
            flush();
            if (stmt != null) stmt.close();
            if (conn != null) conn.close();
        }
    }
}
