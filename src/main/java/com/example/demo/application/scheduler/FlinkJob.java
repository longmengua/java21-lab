package com.example.demo.application.scheduler;

import com.example.demo.application.event.KafkaToClickhouseEvent;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.annotation.PostConstruct;
import lombok.extern.slf4j.Slf4j;
import org.apache.flink.api.common.eventtime.WatermarkStrategy;
import org.apache.flink.api.common.functions.RichMapFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.connector.kafka.source.KafkaSource;
import org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.stereotype.Component;

import java.sql.*;
import java.time.Instant;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicReference;

/***
 * <p>Kafka â†’ Flink â†’ ClickHouse æ‰¹é‡å¯«å…¥ä»»å‹™</p>
 *
 * å•Ÿå‹•æµç¨‹ï¼š
 *   1. Spring Boot å•Ÿå‹•æ™‚æƒæåˆ°æœ¬é¡åˆ¥ï¼ˆ@Componentï¼‰
 *   2. å¯¦ä¾‹åŒ– FlinkJob Bean
 *   3. èª¿ç”¨ @PostConstruct æ¨™è¨»çš„ startFlinkJob()
 *   4. åˆå§‹åŒ– Flink ç’°å¢ƒä¸¦å•Ÿå‹• DataStream ä»»å‹™ï¼ˆéåŒæ­¥ï¼‰
 */
@Component
@ConditionalOnProperty(name = "flink.enabled", havingValue = "true") // åªæœ‰ true æ‰æœƒå»ºç«‹é€™å€‹ Bean
@Slf4j
public class FlinkJob {

    /**
     * å•Ÿå‹• Flink Job
     * åœ¨ Spring å®¹å™¨åˆå§‹åŒ–å¾Œè‡ªå‹•åŸ·è¡Œï¼ˆ@PostConstructï¼‰
     */
    @PostConstruct
    public void startFlinkJob() throws Exception {
        // âœ… é–‹å§‹åŸ·è¡Œ Flink å‰æ‰“å° log
        log.info("âœ… Starting Flink job...");

        // å»ºç«‹ Flink åŸ·è¡Œç’°å¢ƒ
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // è¨­å®šå…¨åŸŸä½µè¡Œåº¦ï¼ˆä¾ CPU æ ¸å¿ƒæ•¸èª¿æ•´ï¼‰
        env.setParallelism(2);

        // å»ºç«‹ Kafka Sourceï¼ˆä½¿ç”¨ Flink Kafka new APIï¼‰
        KafkaSource<String> source = KafkaSource.<String>builder()
                .setBootstrapServers("single-kafka.kafka.orb.local:9092") // Kafka broker
                .setTopics("events") // è¨‚é–± topic
                .setGroupId("flink-consumer") // consumer group
                .setValueOnlyDeserializer(new SimpleStringSchema()) // åªå– valueï¼ˆStringï¼‰
                .setStartingOffsets(OffsetsInitializer.earliest()) // å¾æœ€èˆŠçš„ offset é–‹å§‹
                .build();

        // å¾ Kafka Source å»ºç«‹è³‡æ–™æµï¼ˆraw Stringï¼‰
        DataStream<String> raw = env.fromSource(source, WatermarkStrategy.noWatermarks(), "KafkaSource");

        // JSON â†’ äº‹ä»¶ç‰©ä»¶è½‰æ›
        DataStream<KafkaToClickhouseEvent> parsed = raw.map(new JsonToEvent());

        // å°‡äº‹ä»¶å¯«å…¥ ClickHouseï¼ˆæ‰¹é‡ Sinkï¼‰
        parsed.addSink(new ClickHouseSink());

        // éåŒæ­¥å•Ÿå‹• Job
        env.executeAsync("Spring Boot Flink KafkaToClickHouse (KafkaSource)");
    }

    /**
     * JSON è½‰ KafkaToClickhouseEvent
     * ä½¿ç”¨ RichMapFunction æ–¹ä¾¿æ—¥å¾ŒåŠ ä¸Š open/close ç­‰åˆå§‹åŒ–é‚è¼¯
     */
    static class JsonToEvent extends RichMapFunction<String, KafkaToClickhouseEvent> {
        private final ObjectMapper mapper = new ObjectMapper();

        @Override
        public KafkaToClickhouseEvent map(String json) throws Exception {
            JsonNode node = mapper.readTree(json);
            return new KafkaToClickhouseEvent(
                    node.get("user_id").asLong(),
                    node.get("action").asText(),
                    Instant.parse(node.get("event_time").asText())
            );
        }
    }

    /**
     * ClickHouse Sinkï¼ˆæ‰¹é‡å¯«å…¥ï¼‰
     * - é€é JDBC é€£ç·š ClickHouse
     * - ä½¿ç”¨ buffer æš«å­˜äº‹ä»¶ï¼Œæ»¿ BATCH_SIZE ç­†å†æ‰¹é‡æäº¤
     * - æœƒé¡å¤–å•Ÿå‹•ç›£æ§åŸ·è¡Œç·’ï¼Œæ¯ 5 ç§’è¼¸å‡º buffer ç‹€æ…‹ï¼ˆç©º/ç­†æ•¸ï¼‰
     * âš ï¸ æ³¨æ„ï¼šFlink çš„ç®—å­åœ¨åºåˆ—åŒ–/ååºåˆ—åŒ–æ™‚ï¼Œé transient æ¬„ä½éœ€è¦å¯åºåˆ—åŒ–ã€‚
     *         å› æ­¤èˆ‡å¤–éƒ¨è³‡æºï¼ˆConnectionã€ExecutorServiceï¼‰ç›¸é—œæ¬„ä½ä¸€å¾‹æ¨™è¨˜ç‚º transientï¼Œ
     *         ä¸¦åœ¨ open()/close() ç”Ÿå‘½é€±æœŸä¸­å»ºç«‹èˆ‡é‡‹æ”¾ã€‚
     */
    static class ClickHouseSink extends RichSinkFunction<KafkaToClickhouseEvent> {
        // --- å¤–éƒ¨è³‡æºï¼ˆåœ¨ open() å»ºç«‹ï¼Œåœ¨ close() é‡‹æ”¾ï¼‰ ---
        private transient Connection conn;                 // ClickHouse JDBC é€£ç·š
        private transient PreparedStatement stmt;          // é ç·¨è­¯ SQL
        private transient ScheduledExecutorService monitorExecutor; // ç›£æ§æ’ç¨‹å™¨
        private final transient AtomicReference<ScheduledFuture<?>> monitorFutureRef = new AtomicReference<>();

        // --- æ‰¹æ¬¡ç·©è¡ ---
        private final List<KafkaToClickhouseEvent> buffer = new ArrayList<>(); // æš«å­˜äº‹ä»¶
        private static final int BATCH_SIZE = 100; // æ¯æ‰¹å¯«å…¥ç­†æ•¸

        @Override
        public void open(Configuration parameters) throws Exception {
            // å»ºç«‹ ClickHouse é€£ç·š
            // å¦‚éœ€å¸³å¯†æˆ–é€£ç·šæ± ï¼Œè«‹æ”¹ç‚ºä½¿ç”¨ DataSource ä¸¦å¾å¤–éƒ¨æ³¨å…¥
            conn = DriverManager.getConnection("jdbc:clickhouse://localhost:8123/default");
            //ï¼ˆå¯é¸ï¼‰é—œé–‰ auto-commit ä»¥æå‡æ‰¹æ¬¡æ€§èƒ½ï¼›ClickHouse JDBC é€šå¸¸æœƒå¿½ç•¥ï¼Œä½†ä¿ç•™èªç¾©
            try { conn.setAutoCommit(true); } catch (Exception ignored) {}

            // é ç·¨è­¯ SQLï¼ˆé¿å…é‡è¤‡è§£æï¼‰
            stmt = conn.prepareStatement("INSERT INTO flink_sink (user_id, action, event_time) VALUES (?, ?, ?)");

            // å»ºç«‹ç›£æ§æ’ç¨‹å™¨ï¼ˆå–®åŸ·è¡Œç·’ï¼Œdaemonï¼‰ï¼Œæ¯ 5 ç§’è¼¸å‡ºä¸€æ¬¡ buffer ç‹€æ…‹
            monitorExecutor = Executors.newSingleThreadScheduledExecutor(r -> {
                Thread t = new Thread(r, "ch-buffer-monitor");
                t.setDaemon(true);
                return t;
            });

            // ä¿å­˜ futureï¼Œé—œé–‰æ™‚å¯å–æ¶ˆ
            ScheduledFuture<?> future = monitorExecutor.scheduleAtFixedRate(() -> {
                try {
                    if (buffer.isEmpty()) {
                        log.info("ğŸŸ¡ Still waiting for data from Kafka...");
                    } else {
                        log.info("ğŸ“¦ Buffer size: {}", buffer.size());
                    }
                } catch (Throwable t) {
                    // ç›£æ§ä¸æ‡‰ä¸­æ–·ä¸»æµç¨‹ï¼Œè¨˜éŒ„å³å¯
                    log.warn("Buffer monitor error: {}", t.getMessage(), t);
                }
            }, 0, 5, TimeUnit.SECONDS);
            monitorFutureRef.set(future);
        }

        @Override
        public void invoke(KafkaToClickhouseEvent event, Context context) throws Exception {
            // æ–°äº‹ä»¶åŠ å…¥ buffer
            buffer.add(event);

            // å¦‚æœ buffer æ»¿æ‰¹ï¼Œè§¸ç™¼ flush å¯«å…¥ ClickHouse
            if (buffer.size() >= BATCH_SIZE) {
                flush();
            }
        }

        /**
         * æ‰¹é‡å¯«å…¥ ClickHouse
         * - ä½¿ç”¨ PreparedStatement.addBatch() ç´¯åŠ 
         * - executeBatch() ä¸€æ¬¡é€å‡º
         * - æˆåŠŸå¾Œè¼¸å‡º log
         */
        private void flush() throws SQLException {
            if (buffer.isEmpty()) return; // æ²’è³‡æ–™å°±ç•¥é
            try {
                for (KafkaToClickhouseEvent e : buffer) {
                    stmt.setLong(1, e.getUserId());
                    stmt.setString(2, e.getAction());
                    stmt.setTimestamp(3, Timestamp.from(e.getEventTime()));
                    stmt.addBatch();
                }
                int[] result = stmt.executeBatch();
                //ï¼ˆå¯é¸ï¼‰è‹¥é—œé–‰ auto-commitï¼Œé€™è£¡è¦ conn.commit()
                log.info("âœ… ClickHouse batch insert success: {} records at {}", result.length, Instant.now());
            } catch (SQLException ex) {
                // è©³ç´°è¨˜éŒ„éŒ¯èª¤ï¼Œæ–¹ä¾¿å®šä½å“ªç­†æ•¸æ“šå°è‡´ç•°å¸¸
                log.error("ClickHouse batch insert failed: {}", ex.getMessage(), ex);
                //ï¼ˆå¯é¸ï¼‰å¦‚æœæœ‰ä½¿ç”¨æ‰‹å‹• commitï¼Œå¯è€ƒæ…® rollback
                // try { conn.rollback(); } catch (SQLException ignored) {}
            } finally {
                buffer.clear(); // ç¸½æ˜¯æ¸…ç©º bufferï¼Œé¿å…é‡è¤‡é€
            }
        }

        @Override
        public void close() throws Exception {
            // ä»»å‹™çµæŸå‰ï¼Œå…ˆ flush æœ€å¾Œçš„ buffer
            try {
                flush();
            } catch (Exception e) {
                log.warn("Flush on close error: {}", e.getMessage(), e);
            }

            // 1) é—œé–‰ç›£æ§æ’ç¨‹å™¨ï¼ˆå…ˆå–æ¶ˆä»»å‹™ï¼Œå† shutdownï¼Œå† awaitTerminationï¼‰
            if (monitorExecutor != null && !monitorExecutor.isShutdown()) {
                try {
                    ScheduledFuture<?> f = monitorFutureRef.get();
                    if (f != null && !f.isCancelled()) {
                        f.cancel(false); // ä¸ä¸­æ–·æ­£åœ¨åŸ·è¡Œçš„ä»»å‹™ï¼Œä½†å–æ¶ˆå¾ŒçºŒæ’ç¨‹
                    }
                    monitorExecutor.shutdown();
                    boolean terminated = monitorExecutor.awaitTermination(2, TimeUnit.SECONDS);
                    if (!terminated) {
                        log.warn("Monitor executor did not terminate in time, forcing shutdownNow()");
                        monitorExecutor.shutdownNow();
                    }
                } catch (InterruptedException ie) {
                    // æ¢å¾©ä¸­æ–·ç‹€æ…‹ï¼Œé¿å…åæ‰ä¸­æ–·
                    Thread.currentThread().interrupt();
                    log.warn("Interrupted while awaiting monitor executor termination", ie);
                    monitorExecutor.shutdownNow();
                } catch (Exception e) {
                    log.warn("Error during monitor executor shutdown: {}", e.getMessage(), e);
                }
            }

            // 2) é—œé–‰ JDBC è³‡æºï¼ˆå…ˆ stmt å† connï¼‰
            if (stmt != null) {
                try { stmt.close(); } catch (SQLException e) {
                    log.warn("Error closing PreparedStatement: {}", e.getMessage(), e);
                }
            }
            if (conn != null) {
                try { conn.close(); } catch (SQLException e) {
                    log.warn("Error closing Connection: {}", e.getMessage(), e);
                }
            }
        }
    }
}
