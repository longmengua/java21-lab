package com.example.demo.application.scheduler;

import com.example.demo.infra.props.KafkaProps;
import com.example.demo.infra.props.TopicsProps;
import com.fasterxml.jackson.databind.ObjectMapper;
import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;
import org.springframework.stereotype.Component;

import java.time.Instant;
import java.util.Properties;
import java.util.UUID;
import java.util.concurrent.ThreadLocalRandom;

/**
 * <p>Kafka æ¨¡æ“¬è³‡æ–™ç”Ÿç”¢å™¨</p>
 *
 * å•Ÿå‹•æ¢ä»¶ï¼š
 *   1. application.yml ä¸­ `mock.kafka=true`
 *   2. KafkaProps ä¸­ `kafka.enabled=true`
 *   3. å·²è¨­å®šæœ‰æ•ˆçš„ kafka.bootstrap-servers
 *   4. å·²è¨­å®š topics.events
 *
 * åŠŸèƒ½ï¼š
 *   - å•Ÿå‹•å¾Œè‡ªå‹•åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­ï¼Œæ¯ç§’å›ºå®šé€å‡º N ç­†æ¸¬è©¦è³‡æ–™åˆ°æŒ‡å®š topic
 *   - æŒçºŒ M ç§’ï¼ŒçµæŸå¾Œè‡ªå‹• flush ä¸¦é—œé–‰ producer
 */
@Component
@ConditionalOnProperty(name = "mock.kafka", havingValue = "true") // åªæœ‰ mock.kafka=true æ‰æœƒå»ºç«‹é€™å€‹ Bean
public class KafkaDataProducerJob {

    /** Kafka ç›¸é—œè¨­å®šï¼ˆä¾†è‡ª application.yml çš„ kafka.* å€å¡Šï¼‰ */
    private final KafkaProps kafkaProps;

    /** topics è¨­å®šï¼ˆä¾†è‡ª application.yml çš„ topics.* å€å¡Šï¼‰ */
    private final TopicsProps topics;

    /** JSON åºåˆ—åŒ–å·¥å…·ï¼ˆç”¨æ–¼ç”¢ç”Ÿæ¸¬è©¦è³‡æ–™ JSON å­—ä¸²ï¼‰ */
    private static final ObjectMapper mapper = new ObjectMapper();

    /** Kafka Producer å¯¦ä¾‹ */
    private Producer<String, String> producer;

    /** èƒŒæ™¯é€è³‡æ–™çš„åŸ·è¡Œç·’ */
    private Thread worker;

    /** å»ºæ§‹å­æ³¨å…¥ Kafka èˆ‡ Topics è¨­å®š */
    public KafkaDataProducerJob(KafkaProps kafkaProps, TopicsProps topics) {
        this.kafkaProps = kafkaProps;
        this.topics = topics;
    }

    /**
     * Bean åˆå§‹åŒ–å¾ŒåŸ·è¡Œï¼Œå•Ÿå‹• Mock è³‡æ–™ç”Ÿç”¢æµç¨‹
     */
    @PostConstruct
    public void startProducing() {
        // 0) æª¢æŸ¥ Kafka ç¸½é–‹é—œ
        if (!kafkaProps.isEnabled()) {
            System.out.println("âš  Kafka æœªå•Ÿç”¨ï¼ˆkafka.enabled=falseï¼‰ï¼Œè·³é Mock Producer");
            return;
        }

        // 0.1) æª¢æŸ¥ bootstrap servers æ˜¯å¦è¨­å®š
        String bootstrap = kafkaProps.getBootstrapServers();
        if (bootstrap == null || bootstrap.isBlank()) {
            System.out.println("âŒ ç¼ºå°‘ kafka.bootstrap-serversï¼Œè·³é Mock Producer");
            return;
        }

        // 0.2) æª¢æŸ¥ events topic æ˜¯å¦è¨­å®š
        String topicName = topics.getEvents();
        if (topicName == null || topicName.isBlank()) {
            throw new IllegalStateException("âŒ ç¼ºå°‘ topics.events é…ç½®ï¼Œè«‹åœ¨ application.yml è¨­å®š topics.events");
        }

        // 1) è®€å–æ¸¬è©¦åƒæ•¸ï¼ˆrecords-per-second èˆ‡ duration-secondsï¼‰
        int rps =  (kafkaProps.getProducer() != null) ? kafkaProps.getProducer().getRecordsPerSecond() : 0;
        int seconds = (kafkaProps.getProducer() != null) ? kafkaProps.getProducer().getDurationSeconds() : 0;
        if (rps <= 0 || seconds <= 0) {
            System.out.printf("âš  æ¸¬è©¦åƒæ•¸ä¸ç”Ÿæ•ˆï¼ˆrecordsPerSecond=%d, durationSeconds=%dï¼‰ï¼Œè·³é Mock Producer%n", rps, seconds);
            return;
        }

        // 2) å»ºç«‹ Kafka Producer é€£ç·šè¨­å®š
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrap); // Kafka broker ä½å€
        props.put(ProducerConfig.ACKS_CONFIG, "1"); // leader ç¢ºèªå³å¯
        props.put(ProducerConfig.LINGER_MS_CONFIG, "5"); // æ‰¹æ¬¡å»¶é² 5ms
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, "32768"); // æ‰¹æ¬¡å¤§å°
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4"); // å£“ç¸®æ–¹å¼
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "false"); // æ¨¡æ“¬è³‡æ–™ä¸éœ€è¦ EO
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5"); // æ¯é€£ç·šåŒæ™‚è«‹æ±‚æ•¸
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // key åºåˆ—åŒ–
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // value åºåˆ—åŒ–

        // å»ºç«‹ Producer å¯¦ä¾‹
        this.producer = new KafkaProducer<>(props);

        // 3) å•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’é€è³‡æ–™
        worker = new Thread(() -> {
            System.out.printf("ğŸš€ Mock Kafka Producer ON. Sending %d records/sec to topic '%s' for %d sec...%n",
                    rps, topicName, seconds);

            long endAt = System.currentTimeMillis() + seconds * 1000L;

            while (System.currentTimeMillis() < endAt) {
                long tickStart = System.currentTimeMillis();

                // æ¯ç§’ç™¼é€ rps ç­†è³‡æ–™
                for (int i = 0; i < rps; i++) {
                    try {
                        String key = String.valueOf(randomUserId()); // ä»¥ userId åš keyï¼Œç¢ºä¿åŒ user è½åŒåˆ†å€
                        String json = generateJson(key); // ç”¢ç”Ÿæ¸¬è©¦ JSON
                        producer.send(new ProducerRecord<>(topicName, key, json), (md, ex) -> {
                            if (ex != null) {
                                System.err.println("âŒ Produce failed: " + ex.getMessage());
                            }
                        });
                    } catch (Exception e) {
                        System.err.println("âŒ Build record failed: " + e.getMessage());
                    }
                }

                // ç¯€æµè‡³ 1 ç§’ï¼ˆæ¨¡æ“¬å›ºå®šé€Ÿç‡ï¼‰
                long used = System.currentTimeMillis() - tickStart;
                long sleep = 1000 - used;
                if (sleep > 0) {
                    try { Thread.sleep(sleep); } catch (InterruptedException ignored) {}
                }
            }

            // æ‰€æœ‰è³‡æ–™ç™¼é€å®Œç•¢å¾Œ flush
            producer.flush();
            System.out.println("âœ… Mock Kafka Producer finished.");
        }, "mock-kafka-producer");

        // è¨­ç‚º daemon åŸ·è¡Œç·’ï¼Œéš¨æ‡‰ç”¨çµæŸè‡ªå‹•é—œé–‰
        worker.setDaemon(true);
        worker.start();
    }

    /**
     * Bean éŠ·æ¯€å‰å‘¼å«ï¼Œè² è²¬é‡‹æ”¾è³‡æº
     */
    @PreDestroy
    public void shutdown() {
        try {
            if (worker != null && worker.isAlive()) worker.join(2000); // ç­‰å¾…åŸ·è¡Œç·’çµæŸ
        } catch (InterruptedException ignored) {}
        if (producer != null) producer.close(); // é—œé–‰ Kafka é€£ç·š
    }

    /** ç”¢ç”Ÿéš¨æ©Ÿ userId */
    private static long randomUserId() {
        return ThreadLocalRandom.current().nextLong(10_000L);
    }

    /** ç”¢ç”Ÿæ¸¬è©¦ JSON è³‡æ–™ */
    private static String generateJson(String userId) throws Exception {
        var node = mapper.createObjectNode()
                .put("user_id", userId)
                .put("action", UUID.randomUUID().toString().substring(0, 8))
                .put("event_time", Instant.now().toString());
        return mapper.writeValueAsString(node);
    }
}
